{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderClassifier:\n",
    "    def __init__(self):\n",
    "        self.model = self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False)\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "        x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "        model = tf.keras.Model(inputs=base_model.input, outputs=x)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def predict(self, image):\n",
    "        processed_image = self.preprocess_image(image)\n",
    "        prediction = self.model.predict(processed_image)\n",
    "        gender = 'male' if prediction > 0.5 else 'female'\n",
    "        return gender\n",
    "\n",
    "    def preprocess_image(self, image):\n",
    "        resized_image = tf.image.resize(image, (224, 224))\n",
    "        normalized_image = resized_image / 255.0\n",
    "        return np.expand_dims(normalized_image, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_person_image(image, bbox):\n",
    "    x1, y1, x2, y2 = map(int, bbox)\n",
    "    return image[y1:y2, x1:x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO model and configuration files\n",
    "net = cv2.dnn.readNet(\"D:/vscode/yolov3.weights\", \"D:/vscode/yolov3 (1).cfg\")\n",
    "with open(\"D:/vscode/coco.names\", \"r\") as f:\n",
    "    classes = f.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the car color classification model\n",
    "car_color_model = load_model('car_color_model_best.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming these are the classes used during training\n",
    "class_labels = ['beige', 'black', 'blue', 'brown', 'gold', 'green', 'grey', 'orange', 'pink', 'purple', 'red', 'silver', 'tan', 'white', 'yellow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input image\n",
    "image_path = 'D:/vscode/NULLCLASS Task 2/test4.jpg'\n",
    "image = cv2.imread(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if image is loaded successfully\n",
    "if image is None:\n",
    "    print(f\"Error: Could not load image at {image_path}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 4D blob from the image\n",
    "blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "# Set input blob for the network\n",
    "net.setInput(blob)\n",
    "\n",
    "# Forward pass through the network to get output layers\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "outs = net.forward(output_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for detected bounding boxes, confidences, and class IDs\n",
    "boxes = []\n",
    "confidences = []\n",
    "class_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each detection\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:  # Confidence threshold\n",
    "            # Get bounding box coordinates\n",
    "            center_x = int(detection[0] * image.shape[1])\n",
    "            center_y = int(detection[1] * image.shape[0])\n",
    "            w = int(detection[2] * image.shape[1])\n",
    "            h = int(detection[3] * image.shape[0])\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply non-maxima suppression to eliminate redundant overlapping boxes with lower confidences\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters\n",
    "car_count = 0\n",
    "male_count = 0\n",
    "female_count = 0\n",
    "other_vehicle_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=(64, 64)):\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path, target_size=target_size)\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = image / 255.0  # Normalize the image\n",
    "    return image\n",
    "def predict_car_color(car_color_model, image_path, class_labels):\n",
    "    image = preprocess_image(image_path)\n",
    "    prediction = car_color_model.predict(image)\n",
    "    predicted_class_index = np.argmax(prediction, axis=1)[0]\n",
    "    predicted_class_label = class_labels[predicted_class_index]\n",
    "    return predicted_class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_8664\\3897104568.py:6: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of GenderClassifier\n",
    "gender_classifier = GenderClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n",
      "Car color: white swapped to white\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Car color: white swapped to white\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Car color: white swapped to white\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Car color: white swapped to white\n"
     ]
    }
   ],
   "source": [
    "# Draw bounding boxes and labels on the image\n",
    "if isinstance(indices, np.ndarray) and indices.ndim == 2:\n",
    "    indices = indices.flatten()\n",
    "\n",
    "for i in indices:\n",
    "    box = boxes[i]\n",
    "    x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "    label = str(classes[class_ids[i]])\n",
    "\n",
    "    if label == \"car\":\n",
    "        \n",
    "        car_count += 1\n",
    "        # Extract car image and perform color detection\n",
    "        car_img = image[y:y + h, x:x + w]\n",
    "        color_name = predict_car_color(car_color_model, image_path, class_labels)\n",
    "\n",
    "        # Swap red and blue colors for visualization\n",
    "        if color_name == \"red\":\n",
    "            color_bgr = (255, 0, 0)  # Swap red to blue\n",
    "            swapped_color_name = \"blue\"\n",
    "        elif color_name == \"blue\":\n",
    "            color_bgr = (0, 0, 255)  # Swap blue to red\n",
    "            swapped_color_name = \"red\"\n",
    "        else:\n",
    "            color_bgr = (0, 255, 0)  # Green for other colors\n",
    "            swapped_color_name = color_name\n",
    "\n",
    "        print(f\"Car color: {color_name} swapped to {swapped_color_name}\")\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color_bgr, 2)\n",
    "        cv2.putText(image, f\"{label}, {swapped_color_name}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color_bgr, 2)\n",
    "    elif label in [\"bus\", \"truck\", \"motorbike\", \"bicycle\"]:\n",
    "        other_vehicle_count += 1\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    elif label == \"person\":\n",
    "        # Extract person image and predict gender\n",
    "        person_img = extract_person_image(image, (x, y, x + w, y + h))\n",
    "        if person_img.size > 0:\n",
    "            gender = gender_classifier.predict(person_img)\n",
    "            if gender == \"male\":\n",
    "                male_count += 1\n",
    "            else:\n",
    "                female_count += 1\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (255, 255, 0), 2)\n",
    "            cv2.putText(image, gender, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car count: 4\n",
      "Male count: 0\n",
      "Female count: 1\n",
      "Other vehicle count: 2\n"
     ]
    }
   ],
   "source": [
    "# Print counts\n",
    "print(f\"Car count: {car_count}\")\n",
    "print(f\"Male count: {male_count}\")\n",
    "print(f\"Female count: {female_count}\")\n",
    "print(f\"Other vehicle count: {other_vehicle_count}\")\n",
    "\n",
    "# Display the output image\n",
    "cv2.imshow(\"Output Image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
